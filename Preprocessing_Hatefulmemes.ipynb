{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-06-05T20:33:21.424298Z",
          "start_time": "2025-06-05T20:23:58.879198Z"
        },
        "id": "initial_id",
        "outputId": "9d6bcf90-8a05-4f8e-bcc3-8630b9dea2b8"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# 1) DETECT DEVICE\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) PREPROCESSING FUNCTION\n",
        "# -----------------------------\n",
        "def preprocess_image(\n",
        "    img_path: str,\n",
        "    target_size=(384, 384),\n",
        "    enhance_contrast=1.5,\n",
        "    enhance_sharpness=2.0,\n",
        "    apply_noise_removal=True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Steps 1–3 (PIL/OpenCV) run on CPU.\n",
        "    Steps 4–6 (convert to tensor, resize, normalize) run on GPU if available.\n",
        "    Returns a CPU tensor in range [-1, +1], shape [3, target_h, target_w].\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------ CPU STEPS: Load & Enhance ------------\n",
        "    # 1. Load image (CPU)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    orig_w, orig_h = image.size\n",
        "    tgt_w, tgt_h = target_size\n",
        "\n",
        "    # 2. Compute scale & pad offsets on CPU\n",
        "    scale = min(tgt_w / orig_w, tgt_h / orig_h)\n",
        "    new_w = int(orig_w * scale)\n",
        "    new_h = int(orig_h * scale)\n",
        "    image_resized = image.resize((new_w, new_h), Image.BICUBIC)\n",
        "\n",
        "    canvas = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
        "    paste_x = (tgt_w - new_w) // 2\n",
        "    paste_y = (tgt_h - new_h) // 2\n",
        "    canvas.paste(image_resized, (paste_x, paste_y))\n",
        "\n",
        "    # 3. Enhance contrast & sharpness (CPU)\n",
        "    contrast_enhancer = ImageEnhance.Contrast(canvas)\n",
        "    im_contrasted = contrast_enhancer.enhance(enhance_contrast)\n",
        "    sharpness_enhancer = ImageEnhance.Sharpness(im_contrasted)\n",
        "    im_sharp = sharpness_enhancer.enhance(enhance_sharpness)\n",
        "\n",
        "    # 4. Optional median blur (CPU via OpenCV)\n",
        "    if apply_noise_removal:\n",
        "        np_im = cv2.cvtColor(np.array(im_sharp), cv2.COLOR_RGB2BGR)\n",
        "        np_im = cv2.medianBlur(np_im, 3)\n",
        "        im_processed = Image.fromarray(cv2.cvtColor(np_im, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        im_processed = im_sharp\n",
        "\n",
        "    # ------------ GPU STEPS: Convert & Normalize ------------\n",
        "    # 5. Convert CPU PIL → float tensor in [0,1], then send to GPU\n",
        "    tensor_cpu = torch.from_numpy(\n",
        "        np.array(im_processed).transpose(2, 0, 1)  # [H,W,3] → [3,H,W], dtype=uint8\n",
        "    ).float() / 255.0                               # now in [0,1] on CPU\n",
        "    tensor = tensor_cpu.to(device)                  # move to GPU if available\n",
        "\n",
        "    # 6. Resize on GPU to exact target_size (this duplicates PIL resizing but ensures GPU usage)\n",
        "    tensor = tensor.unsqueeze(0)  # [1,3,H,W]\n",
        "    tensor = F.interpolate(\n",
        "        tensor,\n",
        "        size=target_size,\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False\n",
        "    )                    # [1,3,target_h,target_w] on GPU\n",
        "    tensor = tensor.squeeze(0)  # [3,target_h,target_w]\n",
        "\n",
        "    # 7. Normalize on GPU → range [-1, +1]\n",
        "    mean = torch.tensor([0.5, 0.5, 0.5], device=device).view(3, 1, 1)\n",
        "    std  = torch.tensor([0.5, 0.5, 0.5], device=device).view(3, 1, 1)\n",
        "    tensor = (tensor - mean) / std\n",
        "\n",
        "    # 8. Move back to CPU before returning/saving\n",
        "    return tensor.cpu()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) SPLIT PROCESSING FUNCTION\n",
        "# -----------------------------\n",
        "def process_split(\n",
        "    split: str,\n",
        "    data_root: str,\n",
        "    output_root: str,\n",
        "    target_size=(384, 384),\n",
        "    enhance_contrast=1.5,\n",
        "    enhance_sharpness=2.0,\n",
        "    apply_noise_removal=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes all images in one split: 'train', 'dev', or 'test'.\n",
        "    For each JSONL line, reads 'img' (filename), 'label', and 'text'.\n",
        "    Skips any image if missing. Saves preprocessed tensor + collects metadata.\n",
        "    \"\"\"\n",
        "    assert split in [\"train\", \"dev\", \"test\"], \"split must be 'train', 'dev', or 'test'\"\n",
        "\n",
        "    # 3.1. Load JSONL\n",
        "    jsonl_path = os.path.join(data_root, f\"{split}.jsonl\")\n",
        "    if not os.path.isfile(jsonl_path):\n",
        "        raise FileNotFoundError(f\"Cannot find {split}.jsonl in {data_root}\")\n",
        "\n",
        "    # 3.2. Paths\n",
        "    img_dir = data_root\n",
        "    split_out = os.path.join(output_root, split)\n",
        "    images_out = os.path.join(split_out, \"images\")\n",
        "    os.makedirs(images_out, exist_ok=True)\n",
        "\n",
        "    filenames = []\n",
        "    labels = []\n",
        "    texts = []\n",
        "\n",
        "    # 3.3. Iterate and preprocess\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f, desc=f\"Preprocessing {split}\", unit=\"img\"):\n",
        "            record = json.loads(line)\n",
        "            img_filename = record.get(\"img\") or record.get(\"img_fn\")\n",
        "            if img_filename is None:\n",
        "                continue\n",
        "\n",
        "            label = int(record.get(\"label\", 0))\n",
        "            text = record.get(\"text\", \"\") or record.get(\"text_only\", \"\")\n",
        "\n",
        "            src_path = os.path.join(img_dir, img_filename)\n",
        "            if not os.path.isfile(src_path):\n",
        "                # Skip missing files silently\n",
        "                continue\n",
        "\n",
        "            # Preprocess and save tensor\n",
        "            tensor = preprocess_image(\n",
        "                src_path,\n",
        "                target_size=target_size,\n",
        "                enhance_contrast=enhance_contrast,\n",
        "                enhance_sharpness=enhance_sharpness,\n",
        "                apply_noise_removal=apply_noise_removal\n",
        "            )\n",
        "\n",
        "            base_name = os.path.splitext(img_filename)[0]\n",
        "            out_path = os.path.join(images_out, base_name + \".pt\")\n",
        "\n",
        "            if not os.path.exists(out_path):\n",
        "              os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "\n",
        "            torch.save(tensor, out_path)\n",
        "\n",
        "            filenames.append(base_name)\n",
        "            labels.append(label)\n",
        "            texts.append(text)\n",
        "\n",
        "    # 3.4. Save metadata\n",
        "    meta = {\n",
        "        \"filenames\": filenames,             # e.g. [\"000001\", \"000002\", …]\n",
        "        \"labels\": torch.tensor(labels),     # shape [N], dtype=torch.long\n",
        "        \"texts\": texts                      # list of captions\n",
        "    }\n",
        "    torch.save(meta, os.path.join(split_out, \"metadata.pt\"))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) MAIN: PROCESS ALL SPLITS\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 4.1. Quick check: does img/ exist and contain files?\n",
        "    DATA_ROOT = \"./HatefulMemes/HatefulMemes\"\n",
        "    OUTPUT_ROOT = \"./HatefulMemes/HatefulMemes/HatefulMemes_processed/\"\n",
        "\n",
        "    img_check = os.path.join(DATA_ROOT, \"img\")\n",
        "    if not os.path.isdir(img_check) or len(os.listdir(img_check)) == 0:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Folder '{img_check}' either does not exist or is empty. \"\n",
        "            \"Place your .jpg files in that directory and re-run.\"\n",
        "        )\n",
        "\n",
        "    # 4.2. Ensure OUTPUT_ROOT exists\n",
        "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "\n",
        "    # 4.3. Preprocess each split\n",
        "    for split_name in [\"train\", \"dev\", \"test\"]:\n",
        "        process_split(\n",
        "            split=split_name,\n",
        "            data_root=DATA_ROOT,\n",
        "            output_root=OUTPUT_ROOT,\n",
        "            target_size=(384, 384),\n",
        "            enhance_contrast=1.5,\n",
        "            enhance_sharpness=2.0,\n",
        "            apply_noise_removal=True\n",
        "        )\n",
        "\n",
        "    print(\"✅ All splits processed. Saved under:\", OUTPUT_ROOT)\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing train: 8500img [07:56, 17.83img/s]\n",
            "Preprocessing dev: 500img [00:28, 17.61img/s]\n",
            "Preprocessing test: 1000img [00:57, 17.45img/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All splits processed. Saved under: ./HatefulMemes/HatefulMemes/HatefulMemes_processed/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d98ba2ce9cd11500"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "d98ba2ce9cd11500"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}